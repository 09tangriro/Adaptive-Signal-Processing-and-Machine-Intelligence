{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hottbox.core import Tensor, TensorTKD, residual_tensor\n",
    "from hottbox.algorithms.decomposition import HOSVD, HOOI\n",
    "from coursework.data import get_image, plot_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "[Return to Table of Contents](./0_Table_of_contents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tucker Decomposition\n",
    "\n",
    "<img src=\"./imgs/TensorTKD.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "In previous [assignment](./2_Efficient_representation_of_multidimensional_arrays.ipynb), you have been provided materials which cover efficient representations of mutlidimensional arrays of data, such as the Tucker form. In this module, you will take a closer look at it and the assiciated computational methods.\n",
    "\n",
    "\n",
    "Any tensor of arbitrarily large order can be decomposed in the Tucker form. As illustrated above, a tensor $\\mathbf{\\underline{X}} \\in \\mathbb{R}^{I \\times J \\times K}$ can be represented as a dense core tensor $\\mathbf{\\underline{G}}$ and a set of factor matrices $\\mathbf{A} \\in \\mathbb{R}^{I \\times Q}, \\mathbf{B} \\in \\mathbb{R}^{J \\times R}$ and $\\mathbf{C} \\in\n",
    "\\mathbb{R}^{K \\times P}$\n",
    "\n",
    "$$\n",
    "\\mathbf{\\underline{X}} = \\mathbf{\\underline{G}} \\times_1 \\mathbf{A} \\times_2 \\mathbf{B} \\times_3 \\mathbf{C} = \\Big[    \\mathbf{\\underline{G}} ;  \\mathbf{A},  \\mathbf{B}, \\mathbf{C}      \\Big]\n",
    "$$\n",
    "\n",
    "\n",
    "On practice, there exist several computational methods to accomplish this all of which are combined into a Tucker Decomposition framework. The two most commonly used algorithms are:\n",
    "1. Higher Order Singular Value Decomposition ([HOSVD](#Higher-Order-Singular-Value-Decomposition-(HOSVD)))\n",
    "1. Higher Order Orthogonal Iteration ([HOOI](#Higher-Order-Orthogonal-Iteration-(HOOI)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Higher Order Singular Value Decomposition (HOSVD)\n",
    "\n",
    "The HOSVD is a special case of the Tucker decomposition, in which all the factor matrices are constrained to be orthogonal. They are computed as truncated version of the left singular matrices of all possible mode-$n$ unfoldings of tensor $\\mathbf{\\underline{X}}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{X}_{(1)} &= \\mathbf{U}_1  \\mathbf{\\Sigma}_1 \\mathbf{V}_1^T \\quad \\rightarrow \\quad \\mathbf{A} = \\mathbf{U}_1[1:R_1]\\\\\n",
    "\\mathbf{X}_{(2)} &= \\mathbf{U}_2  \\mathbf{\\Sigma}_2 \\mathbf{V}_2^T \\quad \\rightarrow \\quad \\mathbf{B} = \\mathbf{U}_2[1:R_2] \\\\\n",
    "\\mathbf{X}_{(3)} &= \\mathbf{U}_3  \\mathbf{\\Sigma}_3 \\mathbf{V}_3^T \\quad \\rightarrow \\quad \\mathbf{C} = \\mathbf{U}_3[1:R_3] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "After factor matrices are obtained, the core tensor $\\mathbf{\\underline{G}}$ is computed as\n",
    "\n",
    "$$\n",
    "\\mathbf{\\underline{G}} = \\mathbf{\\underline{X}} \\times_1 \\mathbf{A}^T \\times_2 \\mathbf{B}^T \\times_3 \\mathbf{C}^T        \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher Order Orthogonal Iteration (HOOI)\n",
    "\n",
    "HOOI algorithm is another special case of the Tuker decomposition. Like HOSVD, it decomposes a tensor into a dense core tensor and orthogonal factor matrices. The difference between the two lies in the fact that in HOOI the factor matrices are optimized iteratively using an Alternating Least Squares (ALS) approach. In other words, the tucker representation $[ \\mathbf{\\underline{G}};\\mathbf{A}^{(1)}, \\mathbf{A}^{(2)}, \\cdots,\\mathbf{A}^{(N)} ]$ of the given tensor $\\mathbf{\\underline{X}}$ is obtained through the HOOI as follows\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\mathbf{\\underline{Y}} = \\mathbf{\\underline{X}} \\times_1 \\mathbf{A}^{(1)T} \\times_2 \\cdots \\times_{n-1} \\mathbf{A}^{(n-1)T} \\times_{n+1} \\mathbf{A}^{(n+1)} \\times \\cdots \\times_N \\mathbf{A}^{(N)} \\\\\n",
    "&\\mathbf{A}^{(n)} \\leftarrow R_n \\text{ leftmost singular vectors of } \\mathbf{Y}_{(n)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The above is repeated until convergence, then the core tensor $\\mathbf{\\underline{G}} \\in \\mathbb{R}^{R_1 \\times R_2 \\times \\cdots \\times R_N}$ is computed as\n",
    "\n",
    "$$\n",
    "\\mathbf{\\underline{G}} = \\mathbf{\\underline{X}} \\times_1 \\mathbf{A}^{(1)T}  \\times_2 \\mathbf{A}^{(2)T} \\times_3 \\cdots  \\times_N \\mathbf{A}^{(N)T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Multi-linear rank\n",
    "\n",
    "The **multi-linear rank** of a tensor $\\mathbf{\\underline{X}} \\in \\mathbb{R}^{I_1 \\times \\cdots \\times I_N}$ is the $N$-tuple $(R_1, \\dots, R_N)$ where each $R_n$ is the rank of the subspace spanned by mode-$n$ fibers, i.e. $R_n = \\text{rank} \\big( \\mathbf{X}_{(n)} \\big)$. Thus, for our order-$3$ tensor the multi-linear rank is $(R_1, R_2, R_3)$. Multi-linear rank provides flexibility in compression and approximation of the original tensor.\n",
    "\n",
    "> **NOTE:** For a tensor of order $N$ the values $R_1, R_2, \\dots , R_N$ are not necessarily the same, whereas, for matrices (tensors of order 2) the equality $R_1 = R_2$ always holds, where $R_1$ and $R_2$ are the matrix column rank and row rank respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing tensor decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tucker representation of a tensor with multi-linear rank=(4, 5, 6).\n",
      "Factor matrices represent properties: ['mode-0', 'mode-1', 'mode-2']\n",
      "With corresponding latent components described by (5, 6, 7) features respectively.\n",
      "\n",
      "\tFactor matrices\n",
      "Mode-0 factor matrix is of shape (5, 4)\n",
      "Mode-1 factor matrix is of shape (6, 5)\n",
      "Mode-2 factor matrix is of shape (7, 6)\n",
      "\n",
      "\tCore tensor\n",
      "This tensor is of order 3 and consists of 120 elements.\n",
      "Sizes and names of its modes are (4, 5, 6) and ['mode-0', 'mode-1', 'mode-2'] respectively.\n"
     ]
    }
   ],
   "source": [
    "# Create tensor\n",
    "I, J, K = 5, 6, 7\n",
    "array_3d = np.random.rand(I * J * K).reshape((I, J, K)).astype(np.float)\n",
    "tensor = Tensor(array_3d)\n",
    "\n",
    "# Initialise algorithm\n",
    "algorithm = HOSVD()\n",
    "\n",
    "# Perform decomposing for selected multi-linear rank\n",
    "ml_rank = (4, 5, 6)\n",
    "tensor_tkd = algorithm.decompose(tensor, ml_rank)\n",
    "\n",
    "# Result preview\n",
    "print(tensor_tkd)\n",
    "\n",
    "print('\\n\\tFactor matrices')\n",
    "for mode, fmat in enumerate(tensor_tkd.fmat):\n",
    "    print('Mode-{} factor matrix is of shape {}'.format(mode, fmat.shape))\n",
    "    \n",
    "print('\\n\\tCore tensor')\n",
    "print(tensor_tkd.core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and reconstruction\n",
    "\n",
    "Tucker representation of an original tensor is almost always an approximation, regardless of which algorithm has been employed for performing decomposition. Thus, relative error of approximation is commonly used in order to evaluate performance of computational methods, i.e. the ratio between a Frobenious norms of residual and original tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error of approximation = 0.21320264561618077\n"
     ]
    }
   ],
   "source": [
    "# Compute residual tensor\n",
    "tensor_res = residual_tensor(tensor, tensor_tkd)\n",
    "\n",
    "# Compute error of approximation\n",
    "rel_error = tensor_res.frob_norm / tensor.frob_norm\n",
    "\n",
    "print(\"Relative error of approximation = {}\".format(rel_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Assigment 1**\n",
    "\n",
    "1. Create a tensor of order 4 with sizes of each mode being defined by prime numbers and  obtain a Tucker representation using HOOI algorithm with multi-linear (4, 10, 6, 2). Then calculation ratio between the number of elements in the original tensor and its Tucker form.\n",
    "\n",
    "1. For a tensor that consists of 1331 elements, which multi-linear rank guarantees a perfect reconstruction from its Tucker form and why. Is such choice reasonable for practical applications?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tensor is of order 4 and consists of 1155 elements.\n",
      "Sizes and names of its modes are (5, 11, 7, 3) and ['mode-0', 'mode-1', 'mode-2', 'mode-3'] respectively.\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor\n",
    "\n",
    "I, J, K, L = 5, 11, 7, 3\n",
    "array = np.random.rand(I * J * K * L).reshape((I, J, K, L)).astype(np.float)\n",
    "tensor = Tensor(array)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tucker representation of a tensor with multi-linear rank=(4, 10, 6, 2).\n",
      "Factor matrices represent properties: ['mode-0', 'mode-1', 'mode-2', 'mode-3']\n",
      "With corresponding latent components described by (5, 11, 7, 3) features respectively.\n",
      "\n",
      "\tFactor matrices\n",
      "Mode-0 factor matrix is of shape (5, 4)\n",
      "Mode-1 factor matrix is of shape (11, 10)\n",
      "Mode-2 factor matrix is of shape (7, 6)\n",
      "Mode-3 factor matrix is of shape (3, 2)\n",
      "\n",
      "\tCore tensor\n",
      "This tensor is of order 4 and consists of 480 elements.\n",
      "Sizes and names of its modes are (4, 10, 6, 2) and ['mode-0', 'mode-1', 'mode-2', 'mode-3'] respectively.\n"
     ]
    }
   ],
   "source": [
    "# Perform decomposition\n",
    "\n",
    "algorithm = HOOI()\n",
    "ml_rank = (4, 10, 6, 2)\n",
    "tensor_tkd = algorithm.decompose(tensor, ml_rank)\n",
    "\n",
    "# Result preview\n",
    "print(tensor_tkd)\n",
    "\n",
    "print('\\n\\tFactor matrices')\n",
    "for mode, fmat in enumerate(tensor_tkd.fmat):\n",
    "    print('Mode-{} factor matrix is of shape {}'.format(mode, fmat.shape))\n",
    "    \n",
    "print('\\n\\tCore tensor')\n",
    "print(tensor_tkd.core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor Size = 1155\n",
      "Tucker Tensor Size = 480\n",
      "Ratio = 2.40625\n"
     ]
    }
   ],
   "source": [
    "# Print ratio\n",
    "print('Original Tensor Size =',tensor.size)\n",
    "print('Tucker Tensor Size =',tensor_tkd.core.size)\n",
    "ratio = tensor.size/tensor_tkd.core.size\n",
    "print('Ratio =',ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to prime decomposition, $1331=11\\times 11\\times 11$. For an order 2 tensor, the rank $R_1$ and $R_2$ should be equal. However, this cannot be done with 1331 elements. Therefore, the best multi-linear rank should be $\\mathbf{\\underline{X}} \\in \\mathbb{R}^{11\\times 11 \\times 11}$. In the practical applications, the main use of the HOSVD and HOOI algorithms is to perform higher order principle component analysis and compression. Therefore, this multi-linear rank is not suitable since there is no compression effect, despite all the information being retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Image compression \n",
    "\n",
    "Color images can be naturally represented as a tensor of order three with the shape `(height x width x channels)` where channels are, for example, Red, Blue and Green (RGB)\n",
    "\n",
    "<img src=\"./imgs/image_to_base_colors.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "By keeping its original structure, allows to apply methods from multi-linear analysis. For instance, we can employ algorithms for Tucker decompositions in order to commress oringinal informaiton by varying values of desired multi-linear rank.\n",
    "\n",
    "```python\n",
    "# Get data in form of a Tensor\n",
    "car = get_image(item=\"car\", view=\"top\")\n",
    "tensor = Tensor(car)\n",
    "\n",
    "# Initialise algorithm and preform decomposition\n",
    "algorithm = HOSVD()\n",
    "tensor_tkd = algorithm.decompose(tensor, rank=(25, 25, 3))\n",
    "\n",
    "# Evaluate result\n",
    "tensor_res = residual_tensor(tensor, tensor_tkd)\n",
    "rel_error = tensor_res.frob_norm / tensor.frob_norm\n",
    "\n",
    "print(\"Relative error of approximation = {}\".format(rel_error))\n",
    "```\n",
    "\n",
    "When can also visually inspect image obtained by reconstructing the Tucker representation\n",
    "```python\n",
    "# Reconstruction\n",
    "tensor_rec = tensor_tkd.reconstruct()\n",
    "\n",
    "# Plot original and reconstructed images side by side\n",
    "plot_tensors(tensor, tensor_rec)\n",
    "```\n",
    "\n",
    "<img src=\"./imgs/car_orig_vs_reconstructed_25_25_3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Assigment 2**\n",
    "For this assignment you are provided with function `get_image()` which requires two parameters: `item` and `view`. The valid values for former are **car** and **apple**, while the latter takes only **side** and **top**. \n",
    "\n",
    "1. Use multi-linear rank equal to `(50, 50, 2)` in order to obtain Tucker representations of images of the car and apple. Analyse results by visually inspecting their reconstructions.\n",
    "\n",
    "1. Use multi-linear rank equal to `(50, 50, 2)` in order to obtain Tucker representations of images of the apple taken from the top and from the side. Analyse results by visually inspecting their reconstructions.\n",
    "\n",
    "1. What would happen to the reconstruction if the value of multi-linear rank corresponding to the channel mode is decreased to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tensor is of order 3 and consists of 49152 elements.\n",
      "Sizes and names of its modes are (128, 128, 3) and ['mode-0', 'mode-1', 'mode-2'] respectively. \n",
      "\n",
      "This tensor is of order 3 and consists of 49152 elements.\n",
      "Sizes and names of its modes are (128, 128, 3) and ['mode-0', 'mode-1', 'mode-2'] respectively.\n"
     ]
    }
   ],
   "source": [
    "# Create tensors from images\n",
    "car = get_image(item=\"car\", view=\"top\")\n",
    "tensor_car = Tensor(car)\n",
    "apple = get_image(item=\"apple\", view=\"top\")\n",
    "tensor_apple = Tensor(apple)\n",
    "print(tensor_car,'\\n')\n",
    "print(tensor_apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform decomposition\n",
    "ml_rank = (50, 50, 2)\n",
    "algorithm = HOOI()\n",
    "tensor_car_tkd = algorithm.decompose(tensor_car, ml_rank)\n",
    "tensor_apple_tkd = algorithm.decompose(tensor_apple, ml_rank)\n",
    "print(tensor_car_tkd,'\\n')\n",
    "print(tensor_apple_tkd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results\n",
    "\n",
    "# Compute residual tensor\n",
    "tensor_res_car = residual_tensor(tensor_car, tensor_car_tkd)\n",
    "tensor_res_apple = residual_tensor(tensor_apple, tensor_apple_tkd)\n",
    "\n",
    "# Compute error of approximation\n",
    "rel_error_car = tensor_res_car.frob_norm / tensor_car.frob_norm\n",
    "rel_error_apple = tensor_res_apple.frob_norm / tensor_apple.frob_norm\n",
    "tensor_car_rec = tensor_car_tkd.reconstruct()\n",
    "tensor_apple_rec = tensor_apple_tkd.reconstruct()\n",
    "\n",
    "\n",
    "print(\"Relative error of approximation for car image = {}\".format(rel_error_car))\n",
    "print(\"Relative error of approximation for apple image = {}\".format(rel_error_apple))\n",
    "plot_tensors(tensor_car, tensor_car_rec)\n",
    "plot_tensors(tensor_apple, tensor_apple_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closer the error is to 0 the better the approximation (in a perfect world, the reduced image loses no information). Therefore, the reconstruction of the apple image is 25.3% closer to its original compared to the car reconstruction. Given that both images have been reduced by the same amount, the apple reconstruction is therefore better than the car reconstruction since it retains more of the original information. The original images have dimensions $\\mathbb{R}^{128\\times 128 \\times 3}$, so each image is $128 \\times 128$ pixels with each of the third dimension representing RGB (red, green, blue). The tensor is decomposed to $\\mathbb{R}^{50\\times 50 \\times 2}$. In doing so, the least significant RGB component is lost and the image itself has fewer pixels. \n",
    "\n",
    "In the case of the car, the green information has been lost since there is hardly any green in the image relative to the other colours. This is clearly seen by the framing that turns magenta, which is a combination of blue and red, from white which is a combination of all three colours.\n",
    "\n",
    "In the case of the apple, it appears the red information has been lost since in the reconstructed image, all the red has gone and is replaced by green or black. \n",
    "\n",
    "The apple reconstruction is better since there was less red in the apple than was green in the car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors from images\n",
    "\n",
    "apple_top = get_image(item=\"apple\", view=\"top\")\n",
    "apple_side = get_image(item=\"apple\", view=\"side\")\n",
    "tensor_top = Tensor(apple_top)\n",
    "tensor_side = Tensor(apple_side)\n",
    "print(tensor_top,'\\n')\n",
    "print(tensor_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform decomposition\n",
    "ml_rank = (50, 50, 2)\n",
    "algorithm = HOOI()\n",
    "tensor_tkd_top = algorithm.decompose(tensor_top, ml_rank)\n",
    "tensor_tkd_side = algorithm.decompose(tensor_side, ml_rank)\n",
    "print(tensor_tkd_top,'\\n')\n",
    "print(tensor_tkd_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results\n",
    "\n",
    "# Compute residual tensor\n",
    "tensor_res_top = residual_tensor(tensor_top, tensor_tkd_top)\n",
    "tensor_res_side = residual_tensor(tensor_side, tensor_tkd_side)\n",
    "\n",
    "# Compute error of approximation\n",
    "rel_error_top = tensor_res_top.frob_norm / tensor_top.frob_norm\n",
    "rel_error_side = tensor_res_side.frob_norm / tensor_side.frob_norm\n",
    "tensor_top_rec = tensor_tkd_top.reconstruct()\n",
    "tensor_side_rec = tensor_tkd_side.reconstruct()\n",
    "\n",
    "\n",
    "print(\"Relative error of approximation for apple image (top) = {}\".format(rel_error_top))\n",
    "print(\"Relative error of approximation for apple image (side) = {}\".format(rel_error_side))\n",
    "plot_tensors(tensor_top, tensor_top_rec)\n",
    "plot_tensors(tensor_side, tensor_side_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error of the reconstructed top image is 43.8% better than the reconstructed top image. Again, the original images have dimensions $\\mathbb{R}^{128\\times 128 \\times 3}$ so each image is $128 \\times 128$ pixels with each of the third dimension representing RGB (red, green, blue). Again The tensor is decomposed to $\\mathbb{R}^{50\\times 50 \\times 2}$.\n",
    "\n",
    "From the top image, the colour red has the least significant contribution and is again removed, being replaced by green and black (and absense of all three colours, represented by (0,0,0)).\n",
    "\n",
    "For the side image, there is a better balance between the three primary colours in the original image. Therefore, the colour of the apple is adjusted to the combination of red and green, giving a brown colour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the multi-linear rank corresponding to the channel is reduced to 1, the ouput approximation will be similar to grey-scale but expressed by the dominant colour present in the original image. Therefore, the most significant single channel component is obtained.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpm-coursework",
   "language": "python",
   "name": "dpm-coursework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
